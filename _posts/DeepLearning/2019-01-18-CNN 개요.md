---
title: "CNN 개요"
categories: DeepLearning
---
CNN(Convolutional Neural Network)은 이미지 인식에서 주로 사용되며 NLP, 음성 인식 등 다양한 분야에서도 적용이 가능한 기술입니다. 우리가 보통 흔히 아는 완전연결(Fully Connected)되어 있는 Network을 사용하면 안될까? 라는 생각을 해볼 수 있습니다. <br>
그런데 완전연결 계층에 문제가 있습니다. **데이터의 형상이 무시** 된다는 것입니다. 3차원 데이터를 input으로 전달하기 위해서는 1차원 배열 형태로 전달을 해주어야 하는데 이때 공간적 정보가 손실되게 됩니다. 반면 CNN은 이 형상을 유지하며 input을 3차원 데이터 형태로 받게 됩니다. 

<img src="/assets/images/CNN.gif">

CNN은 입력 데이터를 Filter의 Window를 일정 간격으로 이동해가며 연산을 수행하게 됩니다. 그 결과를 Output 해당 정해진 장소에 저장하게 됩니다. 이 과정을 계속 반복하여 마지막까지 수행하게 되면 CNN의 Output이 완성됩니다.

## Filter(Knerl)
명칭은 문헌에 따라 Filter를 Knerl이라 부르기도 합니다. 다음 그림을 보고 설명하겠습니다.

<img src="/assets/images/filter1.png"><br>
<img src="/assets/images/filter2.png"><br>
왼쪽의 Input에서 초록색의 Filter 만큼을 계산하여 노랑색의 정해진 지점에 Output을 저장합니다. 여기서 Filter란 Input과 곱해지는 부분을 일컫습니다.

## Channel
대개 이미지 인식에서 데이터 Shape은 컬러 사진 데이터의 shape은 (27, 27, 3)으로 표현, 흑백 사진 데이터의 shape은 (27, 27, 1)표현합니다. 여기서 (Height, Width, Channel) 형태를 갖고있게 됩니다. 컬러이미지는 3 Channel을 갖게 되는데 그 이유는 RGB 데이터 표현한 것입니다. NLP에서 CNN을 사용할 때는 Channel 값을 1로 두고 사용을 합니다.

## Padding
<img src="/assets/images/padding_example.PNG">

위의 예시는 Zero-Padding이라고도 불리우며 끝쪽에 추가하는 것을 Padding이라고 부릅니다.

### 왜 사용할까?
- CNN을 계속 사용하면 이미지가 계속 축소
- 가장자리 픽셀을 한 번 밖에 사용함으로써 이미지의 윤곽 정보를 버림

### 종류
- **Valid**: No Padding
- **Same**: Padding 후 이미지 크기가 기존의 이미지 크기와 같음

### 패딩의 크기
패딩 크기에 관한 수식은 다음과 같습니다. 

$$p = \frac{f-1}{2}$$

패딩의 크기는 대부분 홀수입니다. 대개 홀수를 사용한 이유는 다음과 같습니다.
1. 짝수라면 패딩이 비대칭이 됩니다.
2. 홀수 크기의 필터 경우 중심 위치가 존재합니다.

## Strided

- 필터의 이동횟수를 의미 -> Strided 값만큼 이동<br>
  Ex. 다음 이미지는 Strided 값이 2인 예시입니다.<br>
<img src="/assets/images/no_padding_strides.gif">

- 최종 크기에 관한 수식은 다음과 같습니다.
<sub>소수점이 생기면 대개 내림을 사용하고 필터에 맞춰서 크기가 최대한 정수가 나오도록 만듭니다.</sub>
  
$$(\frac{n+2p-f}{s}+1)\times (\frac{n+2p-f}{s}+1)$$

## Multiple Filters
<img src="/assets/images/multiple_filters.PNG">

위의 그림을 보면 6x6x3 이미지가 존재하고 3x3x3 필터 2개를 이용해 결과를 생산하는 이미지입니다.
이런 진행을 다음과 같은 수식으로 표현될 수 있습니다. 위의 상황과 같이 패딩이 존재하지 않고 스트라이드 1인 경우의 상황입니다.
Convoultion 게산을 할 때 일반적으로 이미지와 필터의 nc는 동일합니다. 결과 값에서 nc`의 경우는 필터의 갯수와 같습니다.

$$n\times n \times nc \enspace \ast \enspace f\times f \times nc \enspace= \enspace(n-f+1)\times (n-f+1)\times n{c}'$$

## Pooling Layer
표현의 크기를 줄여 계산이 빨라지고 특징을 더 잘 검출해줄 수 있습니다. 학습하는 변수가 존재하지 않습니다. 즉, Backpropagation 가능한 변수(W)가 존재하지 않습니다.
또한 대부분 Pooling의 Strided는 Filter 사이즈와 동일하며 겹치지 않도록 Pooling하도록 진행됩니다. Padding이 존재하지만 일반적으로 거의 사용하지 않습니다. 수식은 다음과 같습니다.

$$
\begin{align*}&
f: filter size \newline &
s: stride \newline &
p: padding \newline &
(\frac{n_{h} - f}{s}+1) \times  (\frac{n_w - f}{s}+1)
\end{align*}
$$

### Max Pooling
필터의 부분 중 가장 큰 값을 추출하는 방법입니다. 특징을 더 잘 남기기 때문에 주로 사용되는 방법입니다.
Max Pooling에서는 Padding이 존재하지 않습니다.
<img src="/assets/images/max_pooling.PNG">

### Average Pooling
필터의 부분의 평균을 내어 값을 도출하는 방법입니다.
<img src="/assets/images/average_pooling.PNG">

## 왜 Convolutions을 사용할까?
### 1. 학습 변수가 작기 때문입니다.
<img src="/assets/images/why_cnn.PNG"><br>
기존의 NN을 사용시 $3072 \times 4704$개 Parameter가 필요하게 됩니다. 하지만 CNN을 사용하게 되면 다음과 같은 수식을 진행합니다.

$$
\begin{align*}&
5 \times 5 = 25 + 1 = 26 \newline &
6 \times 26 = 156 
\end{align*}
$$

25에 1을 더하는 이유는 bias을 추가하는 것입니다. 26개 Parameter을 가진 filter가 총 6개 있으므로 총 156 Parameter가 존재합니다.
이전에 NN에서 Parameter을 비교해보면 확연히 줄어든 숫자입니다.

학습 변수가 작은 이유
1. Parameter Sharing -> 같은 filter을 사용하여 계속 연산
2. Sparsity of Connections(희소 연결) -> 32 x 32 이미지 중 5 x 5 만  연결된 Output이 나옵니다. 나머지의 픽셀에 대해 영향을 받지 않게 됩니다. 이럴 경우 Overfitting을 방지할 수 있습니다.

### 2. 이동불변성을 포착가능합니다.
이미지에 약간의 변형이 존재하더라도 이를 알아낼 수 있습니다. 예를 들어 고양이 사진이 몇 픽셀 움직이더라도 고양이 사진으로 인식할 수 있습니다.


--------------------------------

## 1x1 Convolution
- 채널의 수가 많아서 줄이기 위해 사용 (H와 W는 줄어들지 않습니다)
- 비선형성을 더 해주고 하나의 층을 더함으로써 더 복잡한 함수를 학습이 가능
- 인셉션 신경망 구축에 용이

<img src="/assets/images/1x1_cnn.jpg"><br>
위의 그림을 보면 1x1 Convolution을 사용하지 않은 경우에는 120M이라는 매우 큰 비용의 계산을 하게 됩니다. 
1x1 Convolution을 사용하게 되면 12.4M으로 줄어든 계산을 하게 되므로 계산 비용이 줄어들게 됩니다.



### Reference
Coursera:  Convolutional Neural Networks (Andrew Ng)<br>
[Convolutional Neural Networks (CNNs): An Illustrated Explanation](https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-cnns-illustrated-explanation/)<br>
[Convolution arithmetic](https://github.com/vdumoulin/conv_arithmetic)
[Visualizing parts of Convolutional Neural Networks using Keras and Cats](https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59)