---
title: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 논문 정리"
categories: NLP
---

----------------------------------

해당 [논문](https://arxiv.org/abs/1810.04805)을 공부하게 되면서 정리한 포스트입니다. 휴먼 레벨보다 높은 성능을 냈고 NLP 분야에서 가장 핫한 논문입니다. 정리한 순서는 논문과 동일한 구조와 같습니다. 잘 못된 내용이나 수정 사항 혹은 의견이 있으시다면 댓글로 남겨주세요.

## Abstract

- BERT: Bidirectional Encoder Representations from Transformers
- BERT는 pre-train deep bidirectional representations
- BERT는 특정 task를 처리하기 위해 추가적인 architecture 없이 fine-tuning을 통해 State-Of-The-Art 모델을 만들었습니다.


## 1 Introduction

Language을 처리하는데 있어서 pre-training은 여러 task에서 좋은 성능을 보여주고 있습니다. task는 sentence-level부터 token-level까지 넓은 부분에 걸쳐있습니다.

**pre-trained** 방법을 적용하는 데 두 가지 방식 존재
1. **feature-based**<br>
   - 특정 task 아키텍처에 pre-trained language representation을 추가적인 feature로 제공
   - 대표적인 방법: ELMo
2. **fine-tuning**<br>
   - task-specific한 parameter를 최소한으로 하고, pre-trained된 parameter들을 fine-tuning하여 downstream task을 학습
   - 대표적인 방법: OpenAI GPT

이 논문에서 주장하는 것은 "**특히 fine-tuning 접근법에서, 현재 기술들(ELMo, OpenAI GPT 같은)이 심각하게 pre-trained의 힘을 제한합니다.**" 입니다.

OpenAI GPT 단방향

fine-tuning 접근법을 BERT을 이용하여 향상시킬 것입니다. 앞서 언급한 단방향(unidirectional) 제한 문제를 **Masked language model(MLM)**을 통해 해결할 것입니다. 

MLM은 무작위로 input으로부터 몇몇 token을 마스킹처리합니다. 이 목적은 오로지 context 기반으로한 마스킹 처리된 단어를 예측하기 위함입니다. 즉 주변 단어의 context을 보고 masking된 단어를 예측하게 됩니다. 이제는 왼쪽에서 오른쪽으로 pre-training을 하는 것과 달리 left와 right의 context가 융합되었습니다.

또한 Masked language model과 **next sentence prediction**라는 task를 pre-training 과정에 추가합니다.

해당 논문의 기여는 다음과 같습니다.

## Related Work
### 2.1 Feature-based Approaches
Feature-based 접근법에서 word embedding은 현대 NLP 시스템에서 필수적인 부분으로 여겨지고 있습니다. 또한 word embedding뿐만 아니라 sentence embedding, paragraph embedding을 feature로 사용했습니다.<br>
ELMo는 traditional word embedding 연구를 다른 차원으로  일반화시켰습니다. ELMo는 모델로부터 *context-sensitive*을 추출하였습니다. 이에 관련되어 자세한 내용은 [ELMo 논문](http://www.aclweb.org/anthology/N18-1202)을 참고하시길 바랍니다.

### 2.2 Fine-tuning Approaches
최근 Trend는 LM 목적에 맞게 pre-train하는 것 입니다. 이 방법의 이점은 적은 파라미터 수가 사용된다는 것입니다. 최근 이러한 이점을 활용하여 OpenaAI GPT가 많은 sentence-level task에서 SOTA을 달성했습니다.

### 2.3 Transfer Learning from Supervised Data
unsupervised pre-training의 학습 방법의 장점은 거의 무한한 데이터를 활용할 수 있다는 점입니다. 또한 이것은 supervised task에서도 효과적인 transfer을 보여주었습니다.


작성중....


### Reference
[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)<br>
[Google Blog BERT: State-of-Art Pre-training for NLP](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)<br>