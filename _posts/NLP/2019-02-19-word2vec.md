---
title: "Efficient Estimation of Word Representations in Vector Space 논문 정리"
categories: NLP
---

----------------------------------

해당 [논문](https://arxiv.org/pdf/1301.3781.pdf?)을 공부하게 되면서 정리한 포스트입니다. 정리한 순서는 논문과 동일한 구조와 같습니다. 잘 못된 내용이나 수정 사항 혹은 의견이 있으시다면 댓글로 남겨주세요.)

## Abstract
- 매우 큰 데이터 set에서 단어 vector 계산을 위한 아키텍처 모델 제안
- 낮은 계산 비용으로 매우 큰 정확도 향상
- 이러한 Vector는 state-of-the-art(SOTA)을 달성

## Introduction

많은 NLP 시스템과 techniques는 단어를 atomic units으로 취급합니다. 장점은 다음과 같습니다.

- Simplicity
- Robustness
- Observation: 엄청 많은 데이터를 학습시킨 간단한(simple) 모델은 적은 데이터로 학습한 복잡한(complex) 모델보다 성능이 좋음

간단한 techniques에는 제한이 있습니다. 
- 음성 자동 인식을 위한 많은 관련된 영역의 데이터 양은 제한되어 있으며 성능은 그러한 high quality의 음성 데이터 size에 영향을 받음
- 많은 언어에 관한 corpora는 오직 십억 단어 혹은 그보다 적은 데이터를 갖고 있음

따라서 이러한 기본적인 techniques의 Scaling up은 어떠한 진전을 낳지않으며, 향상된 기술에 초점을 맞춰야만 합니다.

### 1.1 Goals of the Paper
이 논문의 주요 목표는 **high-quality word vector 학습을 위해 사용될 수 있는 techniques을 도입하는 것**입니다.

유사한 단어는 서로 가까워질 경향이 있고, 뿐만 아니라 similarity의 multiple degrees을 가질 수 있습니다. 예를 들어 명사는 multiple word engdings을 가질 수 있는데 만약 우리가 vector space의 subspace에서 유사한 단어들을 찾았더라면 비슷한 endings을 갖고있는 단어들을 찾는 것이 가능합니다.

무엇보다 놀랍게도 간단한 구문 규칙을 넘어서는 단어의 유사도를 발견한다는 것입니다. 예를 들어 vector("King") - vector("Man") + vector("Woman") 의 값은 Queen 단어 벡터와 매우 가까운 값이 나온게 됩니다.

단언 간의 선형 규칙을 보전하는 새로운 아키텍처을 개발함으로써 벡터 연산의 정확도를 최대화하려 노력하고 있습니다.

또한 이 논문에서 학습 시간과 정확도는 Word vectors의 차원과 학습 데이터 양에 얼마나 의존하고 있는지 논의할 것 입니다.

### 1.2 Previous Work
해당 챕터에 관련된 내용은 논문을 참고해주시기 바랍니다.

## Model Architecture
많고 다양한 종류의 모델들은 words의 continuous representations을 측정하기 위해 제안되었습니다. 대표적으로 Latent Semantic Analysis(LSA)와 Latent Dirichlet Allocation(LDA)가 있습니다.

이 논문에서 초점을 맞추는 것은 Neural Network로 학습된 words의 distirubted representation입니다. 이 모델은 LSA보다 상당히 우수한 성능을 보여주며, LDA는 많은 데이터 Set에서 계산 비용이 매우 크게 발생합니다.

다른 모델 구조를 비교하기 위하여 완전히 학습된 모델에 접근하기 위해 필요한 파라미터의 수로 모델의 계산 복잡도를 정의합니다. 계산 복잡도를 최소화 하면서 정확도를 최도화를 시도할 것입니다.

Training Complexity는 다음과 같습니다.

$$
O = E \times T \times Q
$$

- E: Training Epoch (범위: 3 ~ 50)
- T: Training set의 단어 수 (10억)
- Q: 각각 모델구조를 위해 정의
- Stochastic Gradient Descent와 Backpropagation 사용


### 2.1 Feedforward Neural Net Language Model (NNLM)
NNLM은 Input Layer, Projection Layer, Hidden Layer, Output Layer로 구성된 Neural Network입니다. Input Layer에서 N개의 이전 단어 1-V 방식으로 인코딩됩니다. V는 여기서 vocabulary(사전)의 크기입니다.

그런 이후에 공유 projection matrix을 사용하여 $N \times D$ 차원을 갖고 있는 projection layer P에 Input Layer가 투영(projected)됩니다. N 입력만 active하기 때문에 projection layer는 상대적으로 cheap operation(연산)입니다.

NNLM은 projection layer가 dense하기 때문에 projection과 hidden layer사이 계산이 복잡해집니다. $N=10$인 경우 projection layer의 size는 500 ~ 2000인 반면 hidden layer의 size는 전형적으로 500 ~ 1000입니다.

Hidden layer는 vocabulary에 있는 모든 words에 대한 확률 분포를 계산하는데 사용되며, $V$ 차원을 갖고있는 Output layer가 결과로 나오게 됩니다.

따라서 각 training example에 대한 Computational Complexity는 다음과 같습니다.

$$
Q = N \times D + N \times D \times H + H \times V
$$

- $H \times V$: 계산 비용이 큰 구간 (아래 해결책 제시)

위의 구간을 피하기 위해 몇몇 Solution이 제안되었습니다.
1. Hierarchical versions of the softmax
2. 학습 동안 models을 normalized하지 않음으로써 normalized models을 회피

Vocabulary의 binary tree 표현으로, 평가되어야 하는 output units의 수가 $log_2(V)$로 내려갈 수 있습니다. 따라서 가장 큰 복잡도는 $N \times D \times H$에서 발생됩니다.

이 논문 모델에서 Vocabulary가 허프만 binary tree로 표현되는 Hierarchical softmax을 사용합니다. word의 빈도수가 NNLM에서 Class을 얻기 위해 잘 작동하는 Observation에 따른다. 허프만 tree는 빈번한 words에 짧은 이진코드를 할당하기 떄문에 평가되어야할 output unit의 수가 줄어듭니다. 

균형 이진 트리는 $log_2(V)$ output이 평가되어야 하지만 Hierarchical softmax을 기반으로한 허프만 Tree는 $log_2(Unigram_perplexity(V))$만 필요로 합니다.

예를 들어 어휘 크기가 100만 단어일 때, 이것은 평가에서 약 2배의 속도 증가를 초래합니다. 이것이 $N \times D \times H$가 Computational bottleneck이므로 NNLM에 있어서 중요한 속도 향상은 아니지만, 이후 hidden layer가 없고 따라서 softmax normalization의 효율성에 크게 의존하는 구조를 제안할 것입니다.

### 2.2 Recurrent Neural Network Language Model (RNNLM)
RNN은 NNLM의 특정 단락의 길이의 필요와 같은 한계들을 극복하기 위해 제안되었습니다.
RNN은 shallow한 neural network보다 효율적으로 좀 더 복잡한 패턴을 표현할 수 있습니다. RNN 구조에서는 Projection Layer을 갖고 있지 않으며 Input, Hddien, Ouput Layer로 구성되어 있습니다.

이 모델의 특별한 점은 시간의 흐름에 따른 연결을 사용하여 자신의 hidden 층에 연결된  Recurrent matrix입니다. 이것은 recurrent model이 short term memory을 가질 수 있도록 합니다. 현재 input과 이전 time step에서의 hidden layer의 state을 기반으로 업데이트 되는 hidden layer state로 과거의 정보를 표현될 수 있기 때문입니다.

각 Training마다 Complexity는 다음과 같습니다.

$$
Q = H \times H + H \times V
$$

- D: hidden layer와 같은 차원을 가지는 word representations
- $H \times V$: hierarchical softmax을 사용하여 $H \times log_2(V)$로 효율적으로 줄일 수 있음
- $H \times H$: 대부분의 Complexity는 여기에서 나오게 됨(여기에 영향을 가장 많이 받음)

### 2.3 Parallel Training of Neural Networks
매우 큰 데이터 set을 모델에 학습시키기 위해서 feedforward NNLM과 이 논문에서 제시하는 새로운 모델을 포함하여 DistBelief라고 불리우는 large-scale distributed framework 위에 몇몇 모델을 구현하였습니다. 

이 Framework는 같은 모델의 여러 replicas을 병렬적으로 동작시키도록 하고 각각 replica는 모든 parameter를 갖고 있는 centralized(중앙 집중식) 서버에서 gradient updates을 동기화합니다.

이런 병렬 학습위해서 Adagrad 방식으로 mini-batch 비동기적 gradient descent을 사용합니다. 이 Framework에서는 100개 이상 모델 복제본을 사용하는 것이 일반적이며 각각은 데이터 센터에서 많은 CPU을 사용하고 있습니다.

## 3 New Log-linear Models
Computational Complexity를 최소화 하기 위해 words의 distributed representations을 학습하기 위해 **두가지 새로운 모델 구조**를 제안했습니다. 이전 섹션에서의 주요 Observation은 모델의 non-linear hidden layer가 가장 큰 계산 복잡도의 원인이라는 것입니다.

Neural Network가 매력적이지만 Neural Network만큼 정확하게 data을 표현할 수 없지만 더 많은 Data로 효율적으로 학습할 수 있는 간단한 모델을 탐구하기로 했습니다.

NNLM은 2가지 단계로 성공적으로 학습할 수 있었습니다.
1. Continuous word vectors은 간단한 모델을 사용하여 학습되고, N-gram NNLM은 이러한 distributed representations word vector에 의해 학습됩니다.
2. Word vectors을 학습에 초점을 맞춘 상당한 양의 작업이 있었지만, 논문 저자는 이런 제안된 접근법이 가장 간단한 방법이라고 여겼습니다.

### 3.1 Continuous Bag-of-Words Model
첫 번째 제안 방법은 feedforward NNLM과 유사합니다. 같은 점은 다음과 같습니다. 

- non-linear hidden layer가 제거
- projection layer는 모든 단어가 공유
- 따라서 모든 단어가 동일한 위치에 투영

이러한 모델을 **bag-of-words model**이라고 부르며, word의 순서가 projection에 어떠한 영향을 미치지 않습니다.

더욱이 여기서는 이후(future)에 나올 word을 사용할 것입니다. 이전과 이후의 4개 단어를 입력으로 log-linear 분류기를 이용하여 current word를 분류해내는데 최고의 성능을 보여주었습니다.

이러한 Training Complexity는 다음과 같습니다.

$$
Q = N \times D + D \times log_2(V)
$$

여기서 standard bag-of-words model과 달리 이 모델은 COBW라고 표현할 것이며, 이것은 문맥(context)의 continuous distributed representation을 사용합니다. 

이 모델은 다음과 같은 형태를 갖고 있습니다. NNLM과 같은 방식으로 input과 projection layer 사이에 weight matrix가 모든 word의 위치를 공유하고 있는 점에 주목하여 보면 되겠습니다.

<img src="/assets/images/cbow.PNG"><br>

### 3.2 Continuous Skip-gram Model
두 번째 모델은 CBOW와 유사합니다. 하지만 문맥을 기반으로 current word을 예측하는 것 대신 같은 문장에서 또 다른 word을 기반으로 word를 분류를 극대화합니다.

Continuous projection layer을 가진 log-linear classifier에 Input으로 Current word을 사용하고, 그리고 특정 범위 안에 있는 current word의 앞 뒤 word을 예측합니다.

범위를 늘리는 것이 word vectors의 질을 향상시킨다는 것을 발견했지만 이것은 Computational Complexity을 증가시킵니다.

가까이 있는 것보다 멀리 떨어진(distant) word는 current word와 대개 관련이 적을 것입니다. Training examples에서 distant 단어들을 덜 추출함으로써 멀리 떨어진(distant) words에 낮은 가중치를 부여했습니다.

Training Complexity는 다음과 같습니다.

$$
Q = C \times (D + D \times log_2(V))
$$

- C: wrods의 최대 거리

만약 C=5라면, <1:C> 범위에서 랜덤하게 숫자 R을 선택합니다. 그리고 current word의 이전과 이후에 나오는 R개 words을 correct label로 사용합니다. 즉, current word을 입력으로 하고 각 $R + R$ 단어를 출력(예측하는)으로 하며, $R \times 2$ word classifications을 실행하게 됩니다.

<img src="/assets/images/skip_gram.PNG"><br>

## 4 Results

작성 중..
